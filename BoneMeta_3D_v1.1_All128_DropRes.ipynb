{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkQzLHipM8Vv"
   },
   "source": [
    "#### Update for v1.1\n",
    "- v1.0 was valid to work\n",
    "- v1.1 added doPrediction for predict whole CT scan as in test setting. \n",
    "\n",
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai\n",
    "# !pip install nibabel\n",
    "# !pip install SimpleITK\n",
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "244xQcIJg0RL"
   },
   "outputs": [],
   "source": [
    "MODEL_TO_LOAD = ''\n",
    "FN_LOSS = 0\n",
    "LEVELS = 6\n",
    "TRAINING_NAME = f'3D_Unet_DropRes_lv{LEVELS}_All192'\n",
    "TB_PREFIX = 'bonemeta_fn_{}'.format(FN_LOSS) + '_{}'.format(TRAINING_NAME)\n",
    "\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "# BATCH_SIZE = 120\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 1500\n",
    "\n",
    "BASE_DIR = '/workspace/BoneMeta_all_128'\n",
    "IMG_FOLDER_NAME = 'images'\n",
    "LABEL_FOLDER_NAME = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr6YOqFNyy3v"
   },
   "source": [
    "# Set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1626888627596,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "kT6WRrXoI2fe",
    "outputId": "64071a62-3a8c-4633-89f9-e6586f678c73"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/JungOhLee/bonemeta_model.git ./git_clone\n",
    "# !mv  -v ./git_clone/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1626890250549,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "EmV2m6-yFWDt",
    "outputId": "11802772-72ea-43c3-9334-124985c66b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_DIR = os.path.join(BASE_DIR, IMG_FOLDER_NAME)\n",
    "LABEL_DIR = os.path.join(BASE_DIR, LABEL_FOLDER_NAME)\n",
    "\n",
    "IMG_FILES = os.listdir(IMG_DIR)\n",
    "LABEL_FILES = os.listdir(LABEL_DIR)\n",
    "\n",
    "def get_img_path(file): \n",
    "    return os.path.join(IMG_DIR, file)\n",
    "\n",
    "def get_label_path(file):\n",
    "    return os.path.join(LABEL_DIR, file)\n",
    "\n",
    "def case_to_file(case):\n",
    "    return case+'.npy'\n",
    "\n",
    "def file_to_case(file_name):\n",
    "    return file_name.split('.')[0]\n",
    "\n",
    "set(IMG_FILES).issubset(LABEL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/BoneMeta_all_128/images 317\n",
      "/workspace/BoneMeta_all_128/labels 317\n"
     ]
    }
   ],
   "source": [
    "print(IMG_DIR, len(IMG_FILES))\n",
    "print(LABEL_DIR, len(LABEL_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(IMG_FILES)-set(LABEL_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # z축 작은 녀석들만 남기기 \n",
    "# SMALL_Z_CASES = ['000251_20190703_chest', '000356_20191024_chest', '000350_20190927_chest', '000397_20200214_chest', 'BH005_20170701_abdomen', 'BH041_20190201_C-T-L Spine (3D)', 'BH005_20170701_chest', '000355_20190901_abdomen', '000022_20180830_chest', '000244_20190501_abdomen', '000332_20191025_chest', '000019_20181018_chest', 'BH011_20190301_Spine^00_C_Spine_Pre_OP (Adult)', '000019_20190613_chest', 'BH017_20191201_abdomen', '000273_20190531_chest', '000404_20200305_chest', '000286_20190701_abdomen', 'SN015_20190901_chest', '000391_20200216_Thoracic Aorta CT Angio+3D (contrast)', '000262_20190501_abdomen', 'SN013_20190401_chest', '000285_20190807_chest', '000362_20191201_abdomen', '000322_20190913_chest', 'BH057_20191227_chest', '000400_20200210_chest', 'BH064_20200605_abdomen', 'BH034_20190901_chest', '000298_20190726_chest', '000311_20190902_chest', '000396_20200201_abdomen', '000223_20190319_chest', 'BH032_20180701_abdomen', '000291_20190701_abdomen', 'BH045_20181001_abdomen', '000296_20190726_chest', 'BH066_20190525_chest', 'BH010_20161101_chest', '000251_20190701_abdomen', 'SN007_20190701_chest', '000193_20190114_chest', '000262_20190529_chest', '000251_20190429_chest', 'BH060_20200413_chest', '000234_20190419_chest', 'BH040_20200101_CT Angio + 3D Pulmonary artery (Embolism) (2)', 'BH070_20160823_chest', '000316_20190627_chest', '000356_20190901_abdomen', '000383_20200121_chest', 'BH052_20190901_chest', '000450_20200512_chest', '000276_20190604_chest', '000364_20191217_chest', 'BH025_20191101_abdomen', 'BH035_20191101_GU Kidney & bladder CT (3D)', 'BH018_20190801_chest', '000269_20190601_abdomen', 'BH008_20190701_chest', '000332_20191001_abdomen', 'BH043_20191001_abdomen', '000260_20190522_chest', 'BH007_20190801_chest', 'BH067_20191017_chest', '000279_20190614_chest', 'SN004_20190901_chest', 'BH042_20180801_C-T-L Spine (3D)', 'BH099_20200806_abdomen', 'BH055_20200328_chest', 'BH001_20190401_chest', '000162_20180131_chest', 'BH054_20191122_abdomen', '000450_20200501_abdomen', 'BH030_20160101_T-L spine (3D)', '000372_20200107_chest', 'BH069_20200505_chest', 'BH012_20180401_abdomen', 'BH058_20180918_chest', '000404_20200201_abdomen', 'BH048_20190501_chest', '000348_20191201_CT Liver (contrast)', 'BH002_20190701_chest', '000376_20200101_abdomen', '000309_20190801_abdomen', 'SN017_20190701_chest', 'BH029_20200101_chest', 'BH059_20201013_abdomen', '000310_20190801_abdomen', '000272_20190614_chest', 'BH027_20191001_L-spine CT (3D)', 'BH064_20200605_chest', '000350_20191001_abdomen', '000232_20190423_chest', 'BH001_20190501_abdomen', 'BH039_20190601_chest', 'SN008_20190901_chest', '000270_20190608_chest', 'BH091_20200104_chest', '000269_20190604_chest', '000331_20190916_chest', '000325_20190919_chest', '000363_20191222_chest', '000260_20190501_CT Liver (contrast)', '000330_20190926_chest', 'BH072_20200219_abdomen', '000324_20190910_chest', 'BH009_20180301_chest', 'BH028_20190801_chest', 'SN029_20200301_chest', 'BH110_20200616_chest', '000401_20200201_T-Spine+3D CT (noncontrast)', 'SN031_20160501_chest', 'SN005_20191101_chest', '000452_20200513_chest', '000021_20181227_chest', '000236_20190401_abdomen', 'SN002_20190801_chest', '000382_20200129_chest', '000301_20190801_Pulmonary artery CT Angio+3D (contrast)', 'BH015_20190101_chest', 'BH043_20191001_chest', 'BH063_20200519_chest', '000363_20191201_abdomen', '000354_20191001_abdomen', 'BH047_20180901_abdomen', '000079_20180911_Pulmonary artery CT Angio+3D (contrast)', 'BH016_20151001_chest', 'BH061_20190315_abdomen', 'BH014_20181201_chest', '000362_20191214_chest', '000354_20191022_chest', 'SN051_20170401_chest', 'SN036_20190601_chest', 'BH017_20191201_chest', '000282_20190701_abdomen', 'BH023_20191101_chest', '000085_20180829_chest', 'SN019_20190801_chest', '000212_20190324_chest', '000302_20190726_chest', 'BH009_20180301_abdomen', 'BH008_20190701_abdomen', '000301_20190801_abdomen', 'BH015_20190101_abdomen', 'SN028_20160801_chest', '000344_20191125_chest', 'BH014_20181201_abdomen', '000214_20190325_chest', '000386_20200204_chest', '000372_20200101_abdomen', 'BH091_20200104_GU Kidney & bladder CT (3D)', 'SN025_20200401_chest', '000400_20200201_CT Biliary (contrast)', '000255_20190418_chest', 'BH081_20190322_abdomen', '000315_20190820_chest', '000288_20190701_abdomen', '000272_20190601_abdomen', '000322_20190901_abdomen', '000331_20190901_abdomen', 'SN055_20170301_chest', 'BH052_20190901_abdomen', '000310_20190812_chest', '000069_20180319_chest', 'BH024_20190501_abdomen', 'BH021_20181001_abdomen', '000009_20180417_chest', 'BH037_20171101_abdomen', '000002_20180829_chest', '000234_20190401_abdomen', '000262_20190318_chest', 'SN016_20190901_chest', '000232_20190401_abdomen', 'SN056_20170601_Thorax^01_Lung_Cancer_3D (Adult)', '000300_20190801_abdomen', 'BH006_20170801_chest', '000278_20190620_chest', 'BH036_20180301_GU Kidney & bladder CT (3D)', 'BH019_20191101_chest', 'BH004_20191101_chest', '000382_20200101_abdomen', '000242_20190409_chest', 'BH061_20190315_chest', '000080_20180911_chest', '000355_20191023_chest', '000364_20191201_abdomen', 'BH018_20190801_abdomen', '000291_20190718_chest', '000279_20190601_abdomen', '000308_20190826_chest', '000305_20190801_abdomen', 'BH010_20161101_abdomen', 'BH007_20190801_abdomen', 'BH021_20181001_chest', '000396_20200218_chest', '000091_20180504_chest', 'SN042_20170901_chest', '000314_20190827_chest', 'BH031_20160301_CT Angio + 3D Pulmonary artery (Embolism)', 'BH023_20191101_abdomen', '000301_20190827_chest', 'BH062_20201104_chest', 'BH020_20191201_chest', 'BH059_20201019_chest', '000246_20190629_chest', 'BH026_20190601_abdomen', 'BH040_20200101_CT Angio + 3D Pulmonary artery (Embolism)', 'BH034_20190901_abdomen', 'BH099_20200806_chest', 'BH051_20190301_GU Kidney & bladder CT (3D)', '000368_20200101_abdomen', 'BH057_20191227_abdomen', 'BH112_20190201_chest', 'BH024_20190501_chest', 'BH013_20191001_chest', '000309_20190823_chest', '000011_20181207_chest', 'BH020_20191201_abdomen', 'BH027_20191001_chest', '000316_20190715_Spine^L_SPINE (Adult)', '000012_20181214_chest', 'BH032_20180701_chest', 'BH037_20171101_chest', '000285_20190801_abdomen', 'BH038_20160901_CT angio + 3D C-spine(vertebral artery, C1-2)', 'BH072_20200219_chest', 'SN054_20170201_chest', '000048_20190501_abdomen', '000304_20190124_chest', 'BH045_20181001_chest', '000352_20191001_abdomen', 'BH016_20151001_abdomen', 'BH022_20190101_chest', 'BH047_20180901_chest', '000314_20190901_abdomen', 'BH065_20201013_chest', 'BH056_20200721_chest', '000281_20190701_chest', '000025_20180808_chest', '000352_20191017_chest', 'BH012_20180401_chest', '000224_20190228_chest', '000308_20190801_abdomen', '000376_20200114_chest']\n",
    "# SMALL_Z_FILES = [case_to_file(case) for case in SMALL_Z_CASES]\n",
    "# IMG_FILES = list(set(IMG_FILES).intersection(SMALL_Z_FILES))\n",
    "# len(IMG_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = ['KH027_20210801_Chest(+).npy', 'SN005_20191101_CT_Chest_+_3D_(contr.npy', 'BH091_20200104_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000311_20190902_Chest_CT_(contrast).npy', 'BH023_20191101_Abdomen_&_pelvis_CT_(3D).npy', 'BH076_20180605_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH121_20200301_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000506_20201113_Chest_CT_(contrast).npy', 'BH069_20200505_Chest_CT_(contrast)_+_3D.npy', 'KH039_20210301_Chest_(-)_Routine.npy', 'KH042_20210501_Chest(+).npy', 'BH016_20151001_Abdomen_&_pelvis_CT_(3D).npy', 'BH104_20161219_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000362_20191201_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH035_20191101_GU_Kidney_&_bladder_CT_(3D).npy', '000423_20200401_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'SN036_20190601_CT_Chest_+_3D_(contr.npy', 'BH101_20201109_Chest_CT_(Contrast)_+_3D(TS).npy', 'SN025_20200401_CT_Chest_+_3D_(contr.npy', '000364_20191217_Chest_CT_(contrast).npy', 'BH085_20200801_Abdomen_&_pelvis_CT.npy', 'KH023_20210901_Chest(+).npy', 'BH063_20200519_Chest_CT_(Non_contrast)_+_3D(Breast).npy', '000224_20190228_Chest_CT_(contrast).npy', 'BH116_20180606_GU_Abdomen_&_pelvis_CT_(3D).npy', 'BH023_20191101_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH055_20200328_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'KH002_20210301_Abdomen^00_AbdomenRoutine_(Adult).npy', '000370_20200101_CT_Abdomen+Pelvis_Post_(contrast).npy', '000363_20191222_Chest_3DCT_(contrast).npy', 'BH020_20191201_Chest_CT_(Non_contrast)_+_3D.npy', 'BH015_20190101_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'KH016_20191101_Chest(+).npy', '000262_20190501_CT_Abdomen+Pelvis_Post_(contrast).npy', 'BH081_20190322_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000282_20190701_CT_Abdomen+Pelvis_Post_(contrast).npy', 'SN042_20170901_CT_Chest_+_3D_(contr.npy', 'BH071_20160826_Chest_CT_(contrast)_+_3D.npy', '000356_20191024_Chest_CT_(contrast).npy', '000162_20180131_Chest_CT_(contrast).npy', '000269_20190601_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH119_20200303_Abdomen_&_pelvis_CT_(3D).npy', 'BH043_20191001_Abdomen_&_pelvis_CT_(3D).npy', 'SN031_20160501_2-CT_Chest_Lung_cancer.npy', '000530_20210428_Chest_CT_(contrast).npy', 'BH005_20170707_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', '000363_20191201_CT_Abdomen+Pelvis_Pre-Post_(contrast).npy', 'KH001_20210501_Chest(+).npy', 'KH031_20210801_Chest(+).npy', 'KH010_20201201_Chest(+).npy', '000370_20200117_Chest_CT_(contrast).npy', 'BH037_20171125_Abdomen_&_pelvis_CT_(3D).npy', '000080_20180911_Chest_CT_(contrast).npy', '000262_20190318_Chest_CT_(contrast).npy', 'BH117_20180506_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH034_20190923_GU_OBGY,Abd-pelvis_CT(pre&post)_(3D).npy', 'BH123_20201023_Abdomen_&_pelvis_CT.npy', 'BH061_20190315_Abdomen_&_pelvis_CT_(3D).npy', 'KH007_20200601_Chest(+)_+_Abdomen_&_Pelvis_(+).npy', 'BH028_20190801_Chest_CT_(contrast)_+_3D.npy', '000085_20180829_Chest_CT_(contrast).npy', '000010_20181214_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000435_20200401_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH019_20191101_Thorax^02_Chest_CON_(Adult).npy', '000386_20200204_Chest_CT_(contrast).npy', '000466_20210128_CT_Abdomen+Pelvis_Post_(contrast).npy', 'BH098_20200626_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000325_20190919_Chest_CT_(contrast).npy', '000481_20210305_Thorax^Lung_Pre_(Adult).npy', 'BH108_20180228_Chest_CT_(contrast)_+_3D.npy', 'KH014_20190701_Chest(+).npy', 'SN051_20170401_CT_Chest_+_3D_(contrast_NO_CM).npy', 'BH056_20200721_Chest_CT_(contrast)_+_3D.npy', 'KH014_20190701_IM_Chest(+)_+_Abdomen_&_Pelvis_(+).npy', 'BH091_20200104_GU_Kidney_&_bladder_CT_(3D).npy', 'BH064_20200605_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH070_20160823_Abdomen_&_pelvis_CT_(3D).npy', 'BH089_20200811_Abdomen_&_pelvis_CT.npy', 'KH043_20210601_Thorax^00_Fl_Chest_Routine_(Adult).npy', 'BH002_20190701_Chest_CT_(Non_contrast)_+_3D.npy', '000279_20190614_Chest_3DCT_(contrast).npy', 'BH031_20160301_CT_Angio_+_3D_Pulmonary_artery_(Embolism).npy', 'BH080_20181210_Abdomen_&_pelvis_CT_(3D).npy', 'BH036_20180301_GU_Kidney_&_bladder_CT_(3D).npy', '000272_20190601_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000079_20180911_Pulmonary_artery_CT_Angio+3D_(contrast).npy', '000022_20180830_Chest_CT_(contrast).npy', 'KH026_20210801_Chest(+).npy', 'BH120_20200406_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH073_20201223_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH100_20201007_Thorax^02_Chest_CON_(Adult).npy', 'BH086_20190812_Chest_CT_(Contrast)_+_3D(TS).npy', '000322_20190913_Chest_CT_(contrast).npy', 'BH095_20200727_Abdomen_&_pelvis_CT_(3D).npy', 'BH114_20180920_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'KH016_20191101_Chest(+)_+_Abdomen_&_Pelvis(+).npy', 'SN028_20160801_CT_Abdomen+Pelvis_3D_(contrast).npy', 'SN055_20170301_CT_Chest_Low_Dose_+.npy', 'KH037_20210401_Chest(+).npy', '000309_20190823_Chest_CT_(contrast).npy', 'SN019_20190801_CT_Abdomen+Pelvis_Ar.npy', '000404_20200201_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000528_20210514_Chest_CT_(contrast).npy', '000301_20190827_Chest_CT_(contrast).npy', '000424_20200218_Chest_CT_(contrast).npy', 'BH061_20190315_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH120_20200304_Chest_CT_(contrast)_+_3D.npy', 'BH105_20160820_Chest_CT_(contrast)_+_3D.npy', 'BH123_20201023_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH005_20170701_Abdomen_&_pelvis_CT_(3D).npy', 'BH118_20200617_Liver_CT_(LC_or_CLD,_3D).npy', '000262_20190529_Chest_CT_(contrast).npy', 'BH013_20191001_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000481_20200722_Chest_CT_(noncontrast).npy', 'BH012_20180401_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH054_20191122_Abdomen^00_Liver_CT_(Adult).npy', '000011_20181207_CT_Abdomen+Pelvis_Post_(contrast).npy', 'BH059_20201013_Abdomen_&_pelvis_CT.npy', 'BH082_20200416_Chest_CT_(contrast)_+_3D.npy', 'BH010_20161101_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'BH121_20200301_Abdomen_&_pelvis_CT_(3D).npy', 'SN007_20190701_Thorax^05_Chest_Lung_Cancer_3D_(Adult).npy', '000409_20200310_Chest_CT_(contrast).npy', 'BH111_20161020_CT_Angio_+_3D_Coronary,_Chest.npy', 'SN031_20160501_CT_Abdomen+Pelvis_Ar.npy', 'BH118_20200702_Chest_CT_(contrast)_+_3D.npy', '000560_20210706_Chest_CT_(contrast).npy', '000322_20190901_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000048_20190501_CT_Abdomen+Pelvis_Uro_(contrast).npy', 'SN019_20190801_CT_Chest_+_3D_(contr.npy', '000506_20201110_CT_Urography_(contrast).npy', '000423_20200407_Chest_CT_(contrast).npy', 'BH032_20180701_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'KH033_20210701_Chest(+).npy', 'BH106_20200613_Abdomen_&_pelvis_CT_(3D).npy', 'BH114_20180920_GU_Kidney_&_bladder_CT_(3D).npy', 'BH008_20190701_Abdomen_&_pelvis_CT_(3D).npy', 'BH067_20191017_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000475_20210210_Chest_CT_(contrast).npy', '000417_20200331_Chest_CT_(contrast).npy', 'BH026_20190601_Abdomen_&_pelvis_CT_(3D).npy', '000262_20190319_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH008_20190701_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH015_20190101_Abdomen_&_pelvis_CT_(3D).npy', 'BH090_20200924_Abdomen_&_pelvis_CT.npy', 'BH089_20200803_Chest_CT_(Contrast)_+_3D(TS).npy', '000305_20190801_CT_Acute_Abdomen_(contrast).npy', 'BH106_20200613_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'BH120_20200406_Abdomen_&_pelvis_CT_(3D).npy', '000278_20190620_Chest_CT_(contrast).npy', 'BH012_20180401_Abdomen_&_pelvis_CT_(3D).npy', 'SN002_20190801_CT_Chest_+_3D_(contr.npy', 'BH016_20151001_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000560_20210706_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'KH032_20210701_Chest(+).npy', 'BH064_20200605_Abdomen_&_pelvis_CT_(3D).npy', '000352_20191001_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000424_20200312_Chest_CT_(noncontrast).npy', '000269_20190325_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH107_20200510_Chest_CT_(Contrast)_+_3D(TS).npy', 'SN005_20191001_Abdomen^03_LIVER_(Adult).npy', '000242_20190409_Chest_CT_(contrast).npy', 'BH010_20161101_Abdomen_&_pelvis_CT_(3D).npy', '000291_20190718_Chest_CT_(contrast).npy', '000296_20190726_Chest_CT_(contrast).npy', 'BH081_20190322_Abdomen_&_pelvis_CT_(3D).npy', 'KH030_20210901_Chest_Low-Dose_Screeni.npy', 'KH036_20210401_Chest(+).npy', '000330_20190926_Chest_CT_(contrast).npy', 'KH010_20201201_Chest(+)_+_Abdomen_&_P.npy', '000362_20191214_Chest_CT_(contrast).npy', 'KH028_20210401_Thorax^00_Fl_Chest_Routine_(Adult).npy', 'KH040_20210301_Chest(+).npy', 'SN051_20170401_CT_Abdomen+Pelvis_Arterial+Portal_(contrast).npy', '000400_20200201_CT_Biliary_(contrast).npy', '000368_20200101_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000302_20190726_Chest_CT_(contrast).npy', 'BH048_20190501_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'BH078_20200522_Abdomen_&_pelvis_CT_(3D).npy', '000269_20181016_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000352_20191017_Chest_CT_(contrast).npy', '000356_20190901_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH104_20161219_Abdomen_&_pelvis_CT_(3D).npy', 'BH075_20200721_Chest_CT_(Contrast)_+_3D(Breast).npy', 'BH029_20200101_Chest_HRCT_(lung_cancer).npy', '000069_20180319_Chest_CT_(contrast).npy', '000025_20180808_Chest_CT_(contrast).npy', '000270_20190608_Chest_CT_(contrast).npy', '000409_20200301_CT_Biliary_(contrast).npy', 'BH073_20201223_Abdomen_&_pelvis_CT.npy', 'BH113_20170914_Liver_CT_(LC_or_CLD,_3D).npy', '000212_20190324_Chest_CT_(noncontrast).npy', '000494_20210405_Chest_CT_(contrast).npy', 'BH108_20180223_Liver_CT_(LC_or_CLD,_3D).npy', 'SN042_20170901_CT_Abdomen+Pelvis_3D.npy', '000492_20210401_CT_Abdomen+Pelvis_Pre-Post_(contrast).npy', 'KH025_20211001_Chest(+).npy', '000021_20181227_Chest_CT_(contrast).npy', '000332_20191001_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000391_20200216_Thoracic_Aorta_CT_Angio+3D_(contrast).npy', '000372_20200101_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'SN012_20150801_Abdomen^01_Abdomen_Pelvis_Chest_(Adult).npy', 'SN054_20170201_CT_Chest_Lung_cancer+3D(contrast).npy', '000011_20181207_Chest_CT_(contrast).npy', 'BH070_20160823_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'KH024_20211001_Chest(+).npy', 'BH074_20201113_Abdomen^00_Abd_CT_(Adult).npy', '000291_20190701_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH109_20181210_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', '000376_20200101_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH117_20180506_Abdomen_&_pelvis_CT_(3D).npy', 'BH043_20191001_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'KH009_20200801_Chest(+).npy', '000376_20200114_Chest_CT_(contrast).npy', '000400_20200210_Chest_CT_(contrast).npy', '000269_20181214_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH037_20171125_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'KH007_20200601_Chest(+).npy', '000025_20180808_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000301_20190801_CT_Acute_Abdomen_(contrast).npy', '000309_20190801_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH076_20180605_GU_Abdomen_&_pelvis_CT_(3D).npy', 'BH007_20190801_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000557_20210621_CT_Abdomen+Pelvis_Post_(contrast).npy', 'BH079_20200921_Low_dose_Chest_CT_+_3D(insured).npy', 'BH119_20200227_Chest_CT_(contrast)_+_3D.npy', 'BH066_20190525_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH045_20181001_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000282_20190410_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH051_20190301_GU_Kidney_&_bladder_CT_(3D).npy', '000557_20210621_Chest_CT_(contrast).npy', 'BH098_20200626_GU_Kidney_&_bladder_CT_(3D).npy', '000304_20190124_CT_Liver_(contrast).npy', '000466_20210125_Chest_CT_(contrast).npy', '000272_20190614_Chest_CT_(contrast).npy', 'KH009_20200801_Chest(+)_+_Abdomen_&_Pelvis_(+).npy', '000541_20210520_Chest_CT_(contrast).npy', '000435_20200420_Chest_CT_(contrast).npy', '000481_20210409_CT_Liver_(contrast).npy', 'BH087_20190314_Abdomen_&_pelvis_CT_(3D).npy', '000533_20210507_CT_Liver_(contrast).npy', '000404_20200305_Chest_CT_(contrast).npy', 'BH007_20190801_Abdomen_&_pelvis_CT_(3D).npy', '000308_20190826_Chest_CT_(contrast).npy', 'KH038_20210401_Chest(+).npy', 'BH034_20190901_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000344_20191125_Chest_CT_(contrast).npy', 'BH116_20180606_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'KH002_20210301_Thorax^00_Fl_Chest_Routine_(Adult).npy']\n",
    "VAL_FILES = ['BH099_20200806_Abdomen_&_pelvis_CT.npy', '000019_20181018_Chest_CT_(contrast).npy', 'SN017_20190701_Thorax^05_Chest_Lung_Cancer_3D_(Adult).npy', '000251_20190429_Chest_CT_(contrast).npy', 'SN035_20180101_CT_Stomach+Pelvis_Arterial+Portal(contrast).npy', '000232_20190423_Chest_CT_(contrast).npy', 'BH027_20191001_Chest_CT_(Contrast)_+_3D(TS).npy', 'BH040_20200101_CT_Angio_+_3D_Pulmonary_artery_(Embolism).npy', 'BH024_20190501_Chest_CT_(contrast)_+_3D_(Breast_with_other_CT).npy', 'BH027_20191001_L-spine_CT_(3D).npy', '000286_20190710_[외부_CT_19-07-10]Chest_CT_contrast.npy', '000232_20190401_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000251_20190701_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH099_20200806_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000285_20190807_Chest_CT_(contrast).npy', 'BH024_20190501_Abdomen_&_pelvis_CT_(3D).npy', 'BH021_20181001_Abdomen_&_pelvis_CT_(3D).npy', '000316_20190715_Spine^L_SPINE_(Adult).npy', '000193_20190114_Chest_CT_(contrast).npy', 'SN056_20170601_Thorax^01_Lung_Cancer_3D_(Adult).npy', '000316_20190627_Chest_CT_(contrast).npy', '000251_20190703_Chest_CT_(contrast).npy', 'BH110_20200616_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH006_20170801_Chest_CT_(Contrast)_+_3D(Breast).npy', '000450_20200528_CT_Abdomen+Pelvis_Post_(contrast).npy', 'BH021_20181001_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH065_20201013_Low_dose_Chest_CT_+_3D(insured).npy', '000019_20190601_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', 'BH060_20200413_Chest_CT_(contrast)_+_3D.npy', 'SN016_20190901_CT_Abdomen+Pelvis_Ar.npy', '000450_20200512_Chest_CT_(contrast).npy', '000285_20190801_CT_Abdomen+Pelvis_Pre-Post_(contrast).npy', 'BH062_20201104_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', '000251_20190429_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000019_20190613_Chest_CT_(contrast).npy', 'SN004_20190901_CT_Chest_+_3D_(contrast).npy', '000286_20190701_CT_Abdomen+Pelvis_Dynamic_(contrast).npy']\n",
    "TEST_FILES = ['000214_20190325_Chest_CT_(contrast).npy', '000234_20190401_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000234_20190419_Chest_CT_(contrast).npy', '000236_20190401_CT_Abdomen+Pelvis_Post_(contrast).npy', '000244_20190501_CT_Abdomen+Pelvis_Post_(contrast).npy', '000255_20190418_Chest_CT_(contrast).npy', '000260_20190501_CT_Liver_(contrast).npy', '000260_20190522_Chest_CT_(contrast).npy', '000276_20190604_Chest_CT_(contrast).npy', '000281_20190701_Chest_CT_(contrast).npy', '000288_20190701_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000298_20190726_Chest_CT_(contrast).npy', '000314_20190827_Chest_CT_(contrast).npy', '000314_20190901_CT_Acute_Abdomen_(contrast).npy', '000354_20191001_CT_Abdomen+Pelvis_Post_(contrast).npy', '000354_20191022_Chest_CT_(contrast).npy', '000355_20190901_CT_Abdomen+Pelvis_Dynamic_(contrast).npy', '000355_20191023_Chest_CT_(contrast).npy', '000383_20200121_Chest_CT_(contrast).npy', 'BH001_20190401_Chest_CT_(Non_contrast)_+_3D.npy', 'BH001_20190501_Abdomen_&_pelvis_CT_(3D).npy', 'BH009_20180301_Abdomen_&_pelvis_CT_(3D).npy', 'BH009_20180301_Chest_CT_(contrast)_+_3D_(TS_with_other_CT).npy', 'BH014_20181201_Abdomen_&_pelvis_CT_(3D).npy', 'BH014_20181201_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH017_20191201_Abdomen_&_pelvis_CT_(3D).npy', 'BH017_20191201_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH022_20190101_Thorax^03_Chest_NON_(Adult).npy', 'BH047_20180901_Abdomen_&_pelvis_CT_(3D).npy', 'BH047_20180901_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH052_20190901_Abdomen_&_pelvis_CT_(3D).npy', 'BH052_20190901_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH057_20191227_Abdomen_&_pelvis_CT_(3D).npy', 'BH057_20191227_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'BH058_20180918_Chest_CT_(contrast)_+_3D.npy', 'BH112_20190201_Abdomen_&_pelvis_CT_(3D).npy', 'BH112_20190201_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy', 'SN016_20190901_CT_Chest_+_3D_(contr.npy', 'SN028_20160801_CT_Chest_+_3D_(contrast).npy']\n",
    "\n",
    "# EXCEPTIONS = ['BH040_20200101_CT Angio + 3D Pulmonary artery (Embolism).npy', 'BH040_20200101_CT Angio + 3D Pulmonary artery (Embolism) (2).npy'] # 모델에서 data info 만들때 가끔 오류남.\n",
    "EXCEPTIONS = '''000273_20190531_Chest_CT_(contrast).npy\n",
    "000439_20200624_Chest_CT_(contrast).npy\n",
    "000557_20210621_Chest_CT_(contrast).npy\n",
    "BH002_20190701_Chest_CT_(Non_contrast)_+_3D.npy\n",
    "BH091_20200104_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy\n",
    "BH091_20200104_Chest_CT_(contrast)_+_3D_(Chest_with_other_CT).npy\n",
    "000350_20191001_CT_Abdomen+Pelvis_Dynamic_(contrast).npy\n",
    "000352_20191017_Chest_CT_(contrast).npy\n",
    "000354_20191022_Chest_CT_(contrast).npy\n",
    "000391_20200216_Thoracic_Aorta_CT_Angio+3D_(contrast).npy\n",
    "BH027_20191001_L-spine_CT_(3D).npy\n",
    "000270_20190601_CT_Abdomen+Pelvis_Dynamic_(contrast).npy\n",
    "000270_20190608_Chest_CT_(contrast).npy\n",
    "BH040_20200101_CT_Angio_+_3D_Pulmonary_artery_(Embolism).npy\n",
    "BH118_20200617_Liver_CT_(LC_or_CLD,_3D).npy\n",
    "000424_20200312_Chest_CT_(noncontrast).npy\n",
    "000424_20200218_Chest_CT_(contrast).npy\n",
    "000325_20190919_Chest_CT_(contrast).npy\n",
    "BH069_20200505_Chest_CT_(contrast)_+_3D.npy'''.split('\\n')\n",
    "\n",
    "TRAIN_FILES = list(set(TRAIN_FILES).intersection(set(IMG_FILES)) - set(EXCEPTIONS))\n",
    "VAL_FILES = list(set(VAL_FILES).intersection(set(IMG_FILES)) - set(EXCEPTIONS))\n",
    "TEST_FILES = list(set(TEST_FILES).intersection(set(IMG_FILES)) - set(EXCEPTIONS))\n",
    "\n",
    "# TRAIN_FILES = TRAIN_FILES[:50]\n",
    "# VAL_FILES = VAL_FILES[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = list(set(TRAIN_FILES) - set(VAL_FILES) - set(TEST_FILES))\n",
    "VAL_FILES = list(set(VAL_FILES))\n",
    "TEST_FILES = list(set(TEST_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: 238\n",
      "val: 35\n",
      "test: 30\n",
      "total: 303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "train: {len(TRAIN_FILES)}\n",
    "val: {len(VAL_FILES)}\n",
    "test: {len(TEST_FILES)}\n",
    "total: {len(TRAIN_FILES + VAL_FILES + TEST_FILES)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UNDBJinfPD2q"
   },
   "outputs": [],
   "source": [
    "def adjust_window(image, window):\n",
    "    width = window[0]\n",
    "    level = window[1]\n",
    "    upper = level+width/2\n",
    "    lower = level-width/2\n",
    "    copied_image = image.clip(lower, upper)\n",
    "    copied_image = copied_image-lower\n",
    "    return (copied_image/(upper-lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghAI2GSKIofO"
   },
   "source": [
    "# Set dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import time\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from augmentation import get_transform\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, case_files=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.case_files = case_files\n",
    "        self.images = [np.load(get_img_path(file)) for file in case_files]\n",
    "        self.labels = [np.load(get_label_path(file)) for file in case_files]\n",
    "        self.windows = [(500,200), (700,400), (1200,400)]\n",
    "\n",
    "    def get_data_info(self):\n",
    "        all_data_info = pd.read_csv(f'{BASE_DIR}/data_info_V_2022_04_20.csv')\n",
    "        case_tuple = tuple([file_to_case(file) for file in self.case_files])\n",
    "        include_idx = all_data_info.Case.str.startswith(case_tuple)\n",
    "        return all_data_info.loc[include_idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_case(idx)\n",
    "    \n",
    "    def get_case(self, idx):\n",
    "        patch_image = self.images[idx]\n",
    "        patch_label = self.labels[idx]\n",
    "        return self.process_patch(patch_image, patch_label)\n",
    "\n",
    "    def process_patch(self, patch_image, patch_label):\n",
    "#         patch_image, patch_label = torch.tensor(patch_image, dtype=torch.float32), torch.tensor(patch_label, dtype=torch.bool)\n",
    "#         return self.convert_to_multi_channel_img(patch_image, self.windows), patch_label\n",
    "        augmentation_dict={'flip': True, 'scale':0.2, 'rotate':False,'offset': 0.1, 'noise': 0.1}\n",
    "        transformed_image, transformed_label = get_transform(patch_image, patch_label, augmentation_dict)\n",
    "        return self.convert_to_multi_channel_img(transformed_image, self.windows), transformed_label   \n",
    "    \n",
    "    def convert_to_multi_channel_img(self, image, windows):\n",
    "        adjusted_images = [adjust_window(image, window) for window in windows]\n",
    "        return torch.stack(adjusted_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(self, case_files=None, ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            case_files (string): case filenames\n",
    "        Return:\n",
    "            one whole case\n",
    "\n",
    "        \"\"\"\n",
    "        self.case_files = case_files\n",
    "        self.images = [np.load(get_img_path(file)) for file in case_files]\n",
    "        self.labels = [np.load(get_label_path(file)) for file in case_files]\n",
    "        self.windows = [(500,200), (700,400), (1200,400)]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.images[idx], self.labels[idx]\n",
    "        image_tensor = torch.tensor(image, dtype = torch.float32)\n",
    "        multi_channel_image = self.convert_to_multi_channel_img(image_tensor, self.windows)\n",
    "        return multi_channel_image, torch.tensor(label, dtype = torch.bool)\n",
    "    \n",
    "    def change_to_pytorch_coord(self, coord):\n",
    "        # 현재 데이터 저장이 col, row, z 로 되어있음. -> z, row, col로 변경\n",
    "        return [coord[2],coord[1],coord[0]]\n",
    "\n",
    "    def convert_to_multi_channel_img(self, image, windows):\n",
    "        adjusted_images = [adjust_window(image, window) for window in windows]\n",
    "        return torch.stack(adjusted_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_img(img, offset, end, interval):\n",
    "  if end is None:\n",
    "    end = len(img)\n",
    "  else: \n",
    "    if end > len(img): \n",
    "      end = len(img)\n",
    "    if end <= offset:\n",
    "      end = offset + 1\n",
    "  return img[offset:end:interval]\n",
    "\n",
    "def show_numpy_img(np_img, offset=0, end=None, interval=5, title=''): \n",
    "  sliced_img = slice_img(np_img, offset, end, interval)\n",
    "\n",
    "  figsize_per_img = 3\n",
    "  num_col = 5\n",
    "  num_row = int(np.ceil(sliced_img.shape[0] / num_col))\n",
    "  # fig, axs = plt.subplots(num_row, num_col, figsize = (figsize_per_img*num_col, figsize_per_img*num_row))\n",
    "  plt.figure(figsize=(figsize_per_img*num_col, figsize_per_img*num_row))\n",
    "  for i, img in enumerate(sliced_img):\n",
    "    if i >= num_col*num_row:\n",
    "      continue\n",
    "    # axs[i].imshow(img)\n",
    "    plt.subplot(num_row, num_col, i+1)\n",
    "    plt.imshow(img, 'gray')\n",
    "    # plt.title()\n",
    "  plt.suptitle(title)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "#npy image 정보 보여주기 \n",
    "def show_img_info(np_img):\n",
    "  print('Shape:', np_img.shape)\n",
    "  print('DType:', np_img.dtype)\n",
    "  print('Max:', np_img.max())\n",
    "  print('Min:', np_img.min())\n",
    "  print(np_img)\n",
    "\n",
    "def plot_img_and_label(np_img, label, interval = 5, offset = 0, end=None, figsize_per_image = 5):\n",
    "  sliced_img = slice_img(np_img, offset, end, interval)\n",
    "  sliced_label = slice_img(label, offset, end, interval)\n",
    "  \n",
    "  num_row = len(sliced_img)\n",
    "  num_col = 2\n",
    "\n",
    "  plt.figure(figsize=(figsize_per_image*num_col, figsize_per_image*num_row))\n",
    "\n",
    "  for i in range(0, num_row):\n",
    "    plt.subplot(num_row, num_col, i*num_col+1)\n",
    "    tissue_image = sliced_img[i]\n",
    "    plt.imshow(tissue_image, 'gray')\n",
    "\n",
    "    plt.subplot(num_row, num_col, i*num_col+2)\n",
    "    mask = sliced_label[i]\n",
    "    label_on_tissue = sitk.LabelMapContourOverlay(sitk.Cast(sitk.GetImageFromArray(mask), sitk.sitkLabelUInt8), sitk.GetImageFromArray(tissue_image), opacity=0.7, contourThickness=[2,2], colormap=(0,255,0))\n",
    "    plt.imshow(sitk.GetArrayFromImage(label_on_tissue), 'gray')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, label = train_dataset[0]\n",
    "# plot_img_and_label(img[0].type(torch.int16).numpy(), label.type(torch.uint8).numpy())\n",
    "\n",
    "# # plot_img_and_label(*train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ozUJqsdu6gGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "consumed_time: 1.0386191040743142s\n",
      "train: 238\n",
      "val: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.perf_counter()\n",
    "\n",
    "train_dataset = TrainDataset(TRAIN_FILES)\n",
    "val_dataset = ValDataset(VAL_FILES)\n",
    "\n",
    "elapsed_time = time.perf_counter() - t\n",
    "\n",
    "print(f'''\n",
    "consumed_time: {elapsed_time}s\n",
    "train: {len(train_dataset)}\n",
    "val: {len(val_dataset)}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1626890464593,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "cGOP-nkobJN_",
    "outputId": "73519980-277b-4851-81e4-6fe74bccfe2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7588b08d68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_train = train_dataset[9]\n",
    "plt.imshow(sample_train[0][2][PATCH_SIZE//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1626890465081,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "BhG5M7KJfyx9",
    "outputId": "44e95dda-ebf3-44e2-d662-ab065e2228bc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7588a00588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_train[1][PATCH_SIZE//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IE6jDD9rgy4S"
   },
   "source": [
    "# Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1626890465082,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "szCOieSb6tj3",
    "outputId": "06608c71-88c8-4c2b-cadb-e8e46f91c749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "faTuO1ZW2eN4"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "# NUM_WORKERS = multiprocessing.cpu_count()\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def initTrainDl(train_ds, batch_size = BATCH_SIZE):\n",
    "    if USE_CUDA:\n",
    "        batch_size *= torch.cuda.device_count()\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=USE_CUDA,\n",
    "        shuffle=True,\n",
    "        drop_last=False # to prevent gradient exploding\n",
    "    )\n",
    "    return train_dl\n",
    "\n",
    "def initValDl(val_ds, batch_size = BATCH_SIZE):\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=USE_CUDA,\n",
    "    )\n",
    "    return val_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E26saJkd_CSC"
   },
   "source": [
    "# Set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XFCCB0Vs_tU-"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(\"3DUnet\")\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6M8rm4z0EB9P"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# For logging \n",
    "trn_writer = None\n",
    "val_writer = None\n",
    "# TB_PREFIX = img_type + \"_fn0\"\n",
    "time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lWgh1Szb-obt"
   },
   "outputs": [],
   "source": [
    "# Used for computeClassificationLoss and logMetrics to index into metrics_t/metrics_a\n",
    "# METRICS_LABEL_NDX = 0\n",
    "METRICS_LOSS_NDX = 1\n",
    "# METRICS_FN_LOSS_NDX = 2\n",
    "# METRICS_ALL_LOSS_NDX = 3\n",
    "\n",
    "# METRICS_PTP_NDX = 4\n",
    "# METRICS_PFN_NDX = 5\n",
    "# METRICS_MFP_NDX = 6\n",
    "METRICS_TP_NDX = 7\n",
    "METRICS_FN_NDX = 8\n",
    "METRICS_FP_NDX = 9\n",
    "\n",
    "METRICS_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7752,
     "status": "ok",
     "timestamp": 1626890472808,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "dZA_s68FvDKY",
    "outputId": "a2e787c2-21a9-4760-abbb-c48237c57876"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, AdamW, RMSprop\n",
    "from torch import nn\n",
    "from pytorch3dunet.unet3d.model import ResidualUNet3D, DropResidualUNet3D\n",
    "\n",
    "def initModel():    \n",
    "    segmentation_model = DropResidualUNet3D(in_channels=3, out_channels=1, num_groups=32, f_maps=32, final_sigmoid=True, testing=True, num_levels=LEVELS)\n",
    "\n",
    "    # augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            segmentation_model = nn.DataParallel(segmentation_model)\n",
    "            # augmentation_model = nn.DataParallel(augmentation_model)\n",
    "        segmentation_model = segmentation_model.to(DEVICE)\n",
    "        # augmentation_model = augmentation_model.to(DEVICE)\n",
    "\n",
    "    return segmentation_model #, augmentation_model\n",
    "\n",
    "def initOptimizer():\n",
    "    return AdamW(segmentation_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    # return SGD(segmentation_model.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "segmentation_model = initModel()\n",
    "optimizer = initOptimizer()\n",
    "\n",
    "# Load model\n",
    "if MODEL_TO_LOAD :\n",
    "  model_folder = os.path.join(BASE_DIR, 'models')\n",
    "  model_path = os.path.join(model_folder, MODEL_TO_LOAD)\n",
    "  seg_dict = torch.load(model_path, map_location='cpu')\n",
    "  if torch.cuda.device_count() > 1:\n",
    "    segmentation_model.module.load_state_dict(seg_dict['model_state'])\n",
    "  else:\n",
    "    segmentation_model.load_state_dict(seg_dict['model_state'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): DropResidualUNet3D(\n",
       "    (encoders): ModuleList(\n",
       "      (0): Encoder(\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(1, 3, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Encoder(\n",
       "        (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Encoder(\n",
       "        (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Encoder(\n",
       "        (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Encoder(\n",
       "        (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Encoder(\n",
       "        (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoders): ModuleList(\n",
       "      (0): Decoder(\n",
       "        (upsampling): TransposeConvUpsampling(\n",
       "          (upsample): ConvTranspose3d(1024, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        )\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Decoder(\n",
       "        (upsampling): TransposeConvUpsampling(\n",
       "          (upsample): ConvTranspose3d(512, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        )\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Decoder(\n",
       "        (upsampling): TransposeConvUpsampling(\n",
       "          (upsample): ConvTranspose3d(256, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        )\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Decoder(\n",
       "        (upsampling): TransposeConvUpsampling(\n",
       "          (upsample): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        )\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Decoder(\n",
       "        (upsampling): TransposeConvUpsampling(\n",
       "          (upsample): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        )\n",
       "        (basic_module): DropResNetBlock(\n",
       "          (conv1): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (ReLU): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): SingleConv(\n",
       "            (groupnorm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (non_linearity): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (final_activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141494439"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(segmentation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TeQVaciD_4zd"
   },
   "outputs": [],
   "source": [
    "from util.util import enumerateWithEstimate\n",
    "\n",
    "def doTraining(epoch_ndx, train_dl):\n",
    "    trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device=DEVICE)\n",
    "    segmentation_model.train()\n",
    "    # train_dl.dataset.shuffleSamples() - 정의에서 처리했음\n",
    "\n",
    "    batch_iter = enumerateWithEstimate(\n",
    "        train_dl,\n",
    "        \"E{} Training\".format(epoch_ndx),\n",
    "        start_ndx=train_dl.num_workers,\n",
    "    )\n",
    "    for batch_ndx, batch_tup in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_var = computeBatchLoss(batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
    "        loss_var.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        max_norm = 5\n",
    "        torch.nn.utils.clip_grad_norm_(segmentation_model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "    global totalTrainingSamples_count\n",
    "    totalTrainingSamples_count += trnMetrics_g.size(1)\n",
    "\n",
    "    return trnMetrics_g.to('cpu')\n",
    "\n",
    "def doValidation(epoch_ndx, val_dl):\n",
    "    with torch.no_grad():\n",
    "        valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device=DEVICE)\n",
    "        segmentation_model.eval()\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            val_dl,\n",
    "            \"E{} Validation \".format(epoch_ndx),\n",
    "            start_ndx=val_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            computeBatchLossVal(batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "    return valMetrics_g.to('cpu')\n",
    "\n",
    "def computeBatchLoss(batch_ndx, batch_tup, batch_size, metrics_g,\n",
    "                      classificationThreshold=0.5):\n",
    "    input_t, label_t= batch_tup\n",
    "    \n",
    "    input_g = input_t.to(DEVICE, non_blocking=True)\n",
    "    label_g = label_t.to(DEVICE, non_blocking=True)\n",
    "\n",
    "    # if segmentation_model.training and augmentation_dict:\n",
    "    #     input_g, label_g = augmentation_model(input_g, label_g)\n",
    "\n",
    "    prediction_g_multi_ch = segmentation_model(input_g)\n",
    "    prediction_g = prediction_g_multi_ch[:,0] # B, C, D, H, W -> B, D, H, W\n",
    "    diceLoss_g = diceLoss(prediction_g, label_g)\n",
    "    fnLoss_g = diceLoss(prediction_g * label_g, label_g)\n",
    "    ceLoss = nn.BCELoss()\n",
    "    ceLoss_g = ceLoss(prediction_g, label_g.float())\n",
    "    start_ndx = batch_ndx * batch_size\n",
    "    end_ndx = start_ndx + input_t.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictionBool_g = (prediction_g > classificationThreshold).to(torch.float32)\n",
    "\n",
    "        tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
    "        fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
    "        fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
    "\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
    "        metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "        metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "        metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "\n",
    "    return diceLoss_g.mean() + ceLoss_g + fnLoss_g.mean() * FN_LOSS \n",
    "\n",
    "def computeBatchLossVal(batch_ndx, batch_tup, batch_size, metrics_g,\n",
    "                      classificationThreshold=0.5):\n",
    "    input_t, label_t= batch_tup\n",
    "    \n",
    "    input_g = input_t.to(DEVICE, non_blocking=True)\n",
    "    label_g = label_t.to(DEVICE, non_blocking=True)\n",
    "\n",
    "    prediction_g_multi_ch = segmentation_model(input_g)\n",
    "    prediction_g = prediction_g_multi_ch[:,0] # B, C, D, H, W -> B, D, H, W\n",
    "    diceLoss_g = diceLoss(prediction_g, label_g)\n",
    "    start_ndx = batch_ndx * batch_size\n",
    "    end_ndx = start_ndx + input_t.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictionBool_g = (prediction_g > classificationThreshold).to(torch.float32)\n",
    "\n",
    "        tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
    "        fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
    "        fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
    "\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
    "        metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "        metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "        metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "\n",
    "    return diceLoss_g.mean()\n",
    "\n",
    "def diceLoss(prediction_g, label_g, epsilon=0.00001):\n",
    "    diceLabel_g = label_g.sum(dim=[1,2,3])\n",
    "    dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
    "    diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n",
    "\n",
    "    diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
    "        / (dicePrediction_g + diceLabel_g + epsilon)\n",
    "\n",
    "    return 1 - diceRatio_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UKwQWwI-xoGl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def logMetrics(epoch_ndx, mode_str, metrics_t, tb_type = TB_PREFIX):\n",
    "    log.info(\"E{} {} {}\".format(\n",
    "        epoch_ndx,\n",
    "        \"Unet\",\n",
    "        tb_type\n",
    "    ))\n",
    "\n",
    "    metrics_a = metrics_t.detach().numpy()\n",
    "    sum_a = metrics_a.sum(axis=1)\n",
    "    assert np.isfinite(metrics_a).all()\n",
    "\n",
    "    allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
    "\n",
    "    metrics_dict['percent_all/tp'] = \\\n",
    "        sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100 \n",
    "    metrics_dict['percent_all/fn'] = \\\n",
    "        sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
    "    metrics_dict['percent_all/fp'] = \\\n",
    "        sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
    "\n",
    "\n",
    "    precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
    "        / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
    "    recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] \\\n",
    "        / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
    "\n",
    "    metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
    "        / ((precision + recall) or 1)\n",
    "\n",
    "    log.info((\"E{} {:8} \"\n",
    "              + \"{loss/all:.4f} loss, \"\n",
    "              + \"{pr/precision:.4f} precision, \"\n",
    "              + \"{pr/recall:.4f} recall, \"\n",
    "              + \"{pr/f1_score:.4f} f1 score\"\n",
    "              ).format(\n",
    "        epoch_ndx,\n",
    "        mode_str,\n",
    "        **metrics_dict,\n",
    "    ))\n",
    "    log.info((\"E{} {:8} \"\n",
    "              + \"{loss/all:.4f} loss, \"\n",
    "              + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
    "    ).format(\n",
    "        epoch_ndx,\n",
    "        mode_str + '_all',\n",
    "        **metrics_dict,\n",
    "    ))\n",
    "    global trn_writer\n",
    "    global val_writer\n",
    "    initTensorboardWriters()\n",
    "    if mode_str == 'trn':\n",
    "      writer = trn_writer\n",
    "    elif mode_str == 'pred':\n",
    "      writer = pred_writer\n",
    "    else:\n",
    "      writer = val_writer\n",
    "\n",
    "    prefix_str = 'seg_'\n",
    "\n",
    "    global totalTrainingSamples_count\n",
    "    for key, value in metrics_dict.items():\n",
    "        writer.add_scalar(prefix_str + key, value, totalTrainingSamples_count)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    score = metrics_dict['pr/recall']\n",
    "\n",
    "    return score\n",
    "\n",
    "import os\n",
    "\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "if not os.path.exists(LOG_DIR):\n",
    "  os.mkdir(LOG_DIR)\n",
    "  \n",
    "def initTensorboardWriters():\n",
    "    global trn_writer\n",
    "    global val_writer\n",
    "    global pred_writer\n",
    "    if trn_writer is None:\n",
    "        trn_writer = SummaryWriter(\n",
    "            log_dir= os.path.join(LOG_DIR, '{}_trn_seg_{}').format(TB_PREFIX, time_str) )\n",
    "        val_writer = SummaryWriter(\n",
    "            log_dir= os.path.join(LOG_DIR, '{}_val_seg_{}').format(TB_PREFIX, time_str) )\n",
    "#         pred_writer = SummaryWriter(\n",
    "#             log_dir= os.path.join(LOG_DIR, '{}_pred_seg_{}').format(TB_PREFIX, time_str) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vH1XOnNUHGJn"
   },
   "outputs": [],
   "source": [
    "def saveModel(type_str, epoch_ndx, tb_pre = TB_PREFIX):\n",
    "    model_name = '{}_model_epoch{}'.format(tb_pre, epoch_ndx)\n",
    "    file_path = os.path.join(\n",
    "        BASE_DIR,\n",
    "        'models',\n",
    "        model_name\n",
    "        )\n",
    "\n",
    "    os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
    "\n",
    "    model = segmentation_model\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module\n",
    "\n",
    "    state = {\n",
    "        'sys_argv': sys.argv,\n",
    "        'time': str(datetime.datetime.now()),\n",
    "        'model_state': model.state_dict(),\n",
    "        'model_name': type(model).__name__,\n",
    "        'optimizer_state' : optimizer.state_dict(),\n",
    "        'optimizer_name': type(optimizer).__name__,\n",
    "        'epoch': epoch_ndx,\n",
    "        'totalTrainingSamples_count': totalTrainingSamples_count,\n",
    "    }\n",
    "    torch.save(state, file_path)\n",
    "    log.info(f\"Model was saved to {file_path}\")\n",
    "#     remote_location = 's3://{0}'.format(os.path.join(s3bucket, 'result/models', model_name))\n",
    "#     S3FS.put(file_path, remote_location)\n",
    "#     log.info(\"Saved model params to {} and remote S3 bucket\".format(file_path))\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        log.info(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Pytorch 3D image tensor = Depth, Height, Weight\n",
    "#w(l-> r), h(t->b), d(u->d) 순서로군\n",
    "\n",
    "def pad_for_division(image, patch_size):\n",
    "    patch_d, patch_h, patch_w = patch_size \n",
    "    assert patch_d % 32 == 0 & patch_h % 32 == 0 & patch_w % 32 == 0, \"Patch size should be divided by 32\"\n",
    "    padding_d = patch_d - image.size(0)%patch_d\n",
    "    padding_h = patch_h - image.size(1)%patch_h\n",
    "    padding_w = patch_w - image.size(2)%patch_w\n",
    "    padded_image = F.pad(image, (0, padding_w, 0, padding_h, 0, padding_d))\n",
    "    return padded_image \n",
    "\n",
    "def pad_for_half(image, patch_size):\n",
    "    patch_d, patch_h, patch_w = patch_size \n",
    "    padded_image = F.pad(image, (patch_w//2, patch_w//2, patch_h//2, patch_h//2, patch_d//2, patch_d//2))\n",
    "    return padded_image \n",
    "\n",
    "    \n",
    "def adjust_window(image, window):\n",
    "    width = window[0]\n",
    "    level = window[1]\n",
    "    upper = level+width/2\n",
    "    lower = level-width/2\n",
    "    copied_image = image.clip(lower, upper)\n",
    "    copied_image = copied_image-lower\n",
    "    return (copied_image/(upper-lower))\n",
    "\n",
    "def convert_to_multi_channel_img(image, windows):\n",
    "    adjusted_images = [adjust_window(image, window) for window in windows]\n",
    "    return torch.stack(adjusted_images)\n",
    "\n",
    "# Process : padding -> adjust windows -> unfold -> neural network -> fold -> crop\n",
    "#           another padding -> adjust windows -> unfold -> neural network -> fold -> crop \n",
    "#           average all by 2 -> compare with the label. \n",
    "# 원래는 8개로 해야되는데, 간이 버전이라고 생각해볼 수 있겠음. \n",
    "\n",
    "def pred_image_with_model(padded_image, model, batch_size, patch_size):\n",
    "    '''\n",
    "    padded_image : image tensor with size of [D, H, W]\n",
    "    patch_size : tuple with size of 3\n",
    "    return pred_label : tensor with size of [D, H, W]\n",
    "    '''\n",
    "    windows = [(500,200), (700,400), (1200,400)]\n",
    "    input_channel = padded_image.size(0)\n",
    "    output_channel = 1\n",
    "    patch_d, patch_h, patch_w = patch_size \n",
    "    total_batch_size = batch_size * torch.cuda.device_count()\n",
    "\n",
    "    patches = padded_image.unfold(0, patch_d, patch_d).unfold(1, patch_h, patch_h).unfold(2, patch_w, patch_w)\n",
    "    unfold_shape = patches.size()\n",
    "    patches = patches.reshape(-1, patch_d, patch_h, patch_w)\n",
    "    \n",
    "    processed_patches = torch.zeros_like(patches)\n",
    "    iter_num = int(np.ceil(patches.size(0)/total_batch_size))\n",
    "    for i in range(iter_num):\n",
    "        start = i * total_batch_size\n",
    "        end = (i+1) * total_batch_size\n",
    "        batch = patches[start:end]\n",
    "        batch = convert_to_multi_channel_img(batch, windows)\n",
    "        batch = batch.permute(1,0,2,3,4)\n",
    "        proccessed_batch = model(batch).squeeze(1)\n",
    "        processed_patches[start:end] = proccessed_batch\n",
    "    \n",
    "    pred_patches = processed_patches.view(unfold_shape)\n",
    "    output_d = unfold_shape[0] * unfold_shape[3]\n",
    "    output_h = unfold_shape[1] * unfold_shape[4]\n",
    "    output_w = unfold_shape[2] * unfold_shape[5]\n",
    "    pred_patches = pred_patches.permute(0, 3, 1, 4, 2, 5)\n",
    "    pred_label = pred_patches.reshape(output_d, output_h, output_w)\n",
    "    return pred_label\n",
    "\n",
    "def predict_one_case(image_t, batch_size ,patch_size):\n",
    "    pad_image = pad_for_division(image_t, patch_size)\n",
    "    half_pad_image = pad_for_half(pad_image, patch_size)\n",
    "    d,h,w = image_t.shape\n",
    "    half_pad_d, half_pad_h, half_pad_w = [size//2 for size in patch_size]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        segmentation_model.eval()\n",
    "        pred_label = pred_image_with_model(pad_image, segmentation_model, batch_size, patch_size)\n",
    "        pred_half_pad_label = pred_image_with_model(half_pad_image, segmentation_model, batch_size, patch_size)\n",
    "        \n",
    "    cropped_pred = pred_label[:d, :h, :w]\n",
    "    cropped_pred_2 = pred_half_pad_label[half_pad_d:half_pad_d+d, half_pad_h:half_pad_h+h, half_pad_w:half_pad_w+w]\n",
    "    mean_pred = (cropped_pred + cropped_pred_2) / 2\n",
    "    return mean_pred\n",
    "\n",
    "\n",
    "def doPrediction(epoch_ndx, files, batch_size, patch_size):\n",
    "    log.info(\"E{} Prediction {}\".format(epoch_ndx, TB_PREFIX))\n",
    "    predMetrics_g = torch.zeros(METRICS_SIZE, len(files), device=DEVICE)\n",
    "    segmentation_model.eval()\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        image = np.load(get_img_path(file))\n",
    "        image_t = torch.tensor(image)\n",
    "        label = np.load(get_label_path(file))\n",
    "        \n",
    "        pred = predict_one_case(image_t, batch_size, patch_size)\n",
    "        pred_t = pred > 0.5 # classificationThreshold = 0.5\n",
    "        label_t = torch.tensor(label)\n",
    "        \n",
    "        predictionBool_g = pred_t.unsqueeze(0).to(torch.float32)\n",
    "        label_g = label_t.unsqueeze(0)\n",
    "        \n",
    "        diceLoss_g = diceLoss(predictionBool_g, label_g, epsilon=0.01)\n",
    "        fnLoss_g = diceLoss(predictionBool_g * label_g, label_g)\n",
    "\n",
    "        tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
    "        fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
    "        fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
    "        \n",
    "        predMetrics_g[METRICS_LOSS_NDX, i] = diceLoss_g # 차원 에러날듯 - i로 골라버리면 차원이 하나 줄기 때문.. 확인해봐야함. \n",
    "        predMetrics_g[METRICS_TP_NDX, i] = tp\n",
    "        predMetrics_g[METRICS_FN_NDX, i] = fn\n",
    "        predMetrics_g[METRICS_FP_NDX, i] = fp\n",
    "        \n",
    "    return predMetrics_g.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xamjgc4JFgtg"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 04:13:38,567 INFO     pid:1827895 3DUnet:001:<module> Starting traning...\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Starting traning...\")\n",
    "\n",
    "train_dl = initTrainDl(train_dataset)\n",
    "val_dl = initValDl(val_dataset)\n",
    "\n",
    "best_score = 0.0\n",
    "validation_cadence = 10\n",
    "pred_cadence = 20\n",
    "\n",
    "totalTrainingSamples_count = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 04:13:38,577 INFO     pid:1827895 3DUnet:008:<module> Epoch 1 of 1500, 20/12 batches of size 3*4\n",
      "2022-05-04 04:13:38,580 WARNING  pid:1827895 util.util:221:enumerateWithEstimate E1 Training ----/20, starting\n",
      "2022-05-04 04:14:36,971 INFO     pid:1827895 util.util:241:enumerateWithEstimate E1 Training   16/20, done at 2022-05-04 04:14:43, 0:00:27\n",
      "2022-05-04 04:14:44,105 WARNING  pid:1827895 util.util:252:enumerateWithEstimate E1 Training ----/20, done at 2022-05-04 04:14:44\n",
      "2022-05-04 04:14:44,107 INFO     pid:1827895 3DUnet:007:logMetrics E1 Unet bonemeta_fn_0_3D_Unet_DropRes_lv6_All128\n",
      "2022-05-04 04:14:44,109 INFO     pid:1827895 3DUnet:043:logMetrics E1 trn      0.8118 loss, 0.1471 precision, 0.2786 recall, 0.1925 f1 score\n",
      "2022-05-04 04:14:44,109 INFO     pid:1827895 3DUnet:051:logMetrics E1 trn_all  0.8118 loss,  27.9% tp,  72.1% fn,     161.5% fp\n",
      "2022-05-04 04:14:44,119 WARNING  pid:1827895 util.util:221:enumerateWithEstimate E1 Validation  ----/12, starting\n",
      "2022-05-04 04:14:50,432 WARNING  pid:1827895 util.util:252:enumerateWithEstimate E1 Validation  ----/12, done at 2022-05-04 04:14:50\n",
      "2022-05-04 04:14:50,474 INFO     pid:1827895 3DUnet:007:logMetrics E1 Unet bonemeta_fn_0_3D_Unet_DropRes_lv6_All128\n",
      "2022-05-04 04:14:50,475 INFO     pid:1827895 3DUnet:043:logMetrics E1 val      0.7258 loss, 0.3387 precision, 0.2651 recall, 0.2974 f1 score\n",
      "2022-05-04 04:14:50,476 INFO     pid:1827895 3DUnet:051:logMetrics E1 val_all  0.7258 loss,  26.5% tp,  73.5% fn,      51.8% fp\n",
      "2022-05-04 04:14:56,925 INFO     pid:1827895 3DUnet:026:saveModel Model was saved to /workspace/BoneMeta_all_128/models/bonemeta_fn_0_3D_Unet_DropRes_lv6_All128_model_epoch1\n",
      "2022-05-04 04:14:59,785 INFO     pid:1827895 3DUnet:032:saveModel SHA1: b34345f2a949c7e8c9b3cc948ecd25489d75581b\n",
      "2022-05-04 04:14:59,786 INFO     pid:1827895 3DUnet:008:<module> Epoch 2 of 1500, 20/12 batches of size 3*4\n",
      "2022-05-04 04:14:59,789 WARNING  pid:1827895 util.util:221:enumerateWithEstimate E2 Training ----/20, starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d54358c24f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ))\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrnMetrics_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlogMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_ndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrnMetrics_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-721557a32493>\u001b[0m in \u001b[0;36mdoTraining\u001b[0;34m(epoch_ndx, train_dl)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtotalTrainingSamples_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_ndx in range(1, 51):\n",
    "    log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "        epoch_ndx,\n",
    "        EPOCHS,\n",
    "        len(train_dl),\n",
    "        len(val_dl),\n",
    "        BATCH_SIZE,\n",
    "        (torch.cuda.device_count() if USE_CUDA else 1),\n",
    "    ))\n",
    "\n",
    "    trnMetrics_t = doTraining(epoch_ndx, train_dl)\n",
    "\n",
    "    logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "    if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
    "        # if validation is wanted\n",
    "        valMetrics_t = doValidation(epoch_ndx, val_dl)\n",
    "        score = logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "        best_score = max(score, best_score)\n",
    "\n",
    "        # self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "        saveModel('seg', epoch_ndx)\n",
    "        # self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "        # self.logImages(epoch_ndx, 'val', val_dl)\n",
    "\n",
    "#     if epoch_ndx % pred_cadence ==0:\n",
    "#         predMetrics_t = doPrediction(epoch_ndx, VAL_FILES, batch_size=1, patch_size=(128,256,256))\n",
    "#         score = logMetrics(epoch_ndx, 'pred', predMetrics_t)\n",
    "        \n",
    "trn_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.0001\n",
    "    g['weight_decay'] = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_ndx in range(51, 201):\n",
    "    log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "        epoch_ndx,\n",
    "        EPOCHS,\n",
    "        len(train_dl),\n",
    "        len(val_dl),\n",
    "        BATCH_SIZE,\n",
    "        (torch.cuda.device_count() if USE_CUDA else 1),\n",
    "    ))\n",
    "\n",
    "    trnMetrics_t = doTraining(epoch_ndx, train_dl)\n",
    "\n",
    "    logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "    if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
    "        # if validation is wanted\n",
    "        valMetrics_t = doValidation(epoch_ndx, val_dl)\n",
    "        score = logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "        best_score = max(score, best_score)\n",
    "\n",
    "        # self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "        saveModel('seg', epoch_ndx)\n",
    "        # self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "        # self.logImages(epoch_ndx, 'val', val_dl)\n",
    "\n",
    "#     if epoch_ndx % pred_cadence ==0:\n",
    "#         predMetrics_t = doPrediction(epoch_ndx, VAL_FILES, batch_size=1, patch_size=(128,256,256))\n",
    "#         score = logMetrics(epoch_ndx, 'pred', predMetrics_t)\n",
    "        \n",
    "trn_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.00005\n",
    "    g['weight_decay'] = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_ndx in range(201, 701):\n",
    "    log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "        epoch_ndx,\n",
    "        EPOCHS,\n",
    "        len(train_dl),\n",
    "        len(val_dl),\n",
    "        BATCH_SIZE,\n",
    "        (torch.cuda.device_count() if USE_CUDA else 1),\n",
    "    ))\n",
    "\n",
    "    trnMetrics_t = doTraining(epoch_ndx, train_dl)\n",
    "\n",
    "    logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "    if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
    "        # if validation is wanted\n",
    "        valMetrics_t = doValidation(epoch_ndx, val_dl)\n",
    "        score = logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "        best_score = max(score, best_score)\n",
    "\n",
    "        # self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "        saveModel('seg', epoch_ndx)\n",
    "        # self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "        # self.logImages(epoch_ndx, 'val', val_dl)\n",
    "\n",
    "#     if epoch_ndx % pred_cadence ==0:\n",
    "#         predMetrics_t = doPrediction(epoch_ndx, VAL_FILES, batch_size=1, patch_size=(128,256,256))\n",
    "#         score = logMetrics(epoch_ndx, 'pred', predMetrics_t)\n",
    "        \n",
    "trn_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.00001\n",
    "    g['weight_decay'] = 0.000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_ndx in range(701, EPOCHS + 1):\n",
    "    log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "        epoch_ndx,\n",
    "        EPOCHS,\n",
    "        len(train_dl),\n",
    "        len(val_dl),\n",
    "        BATCH_SIZE,\n",
    "        (torch.cuda.device_count() if USE_CUDA else 1),\n",
    "    ))\n",
    "\n",
    "    trnMetrics_t = doTraining(epoch_ndx, train_dl)\n",
    "\n",
    "    logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "    if epoch_ndx == 1 or epoch_ndx % validation_cadence == 0:\n",
    "        # if validation is wanted\n",
    "        valMetrics_t = doValidation(epoch_ndx, val_dl)\n",
    "        score = logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "        best_score = max(score, best_score)\n",
    "\n",
    "        # self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "        saveModel('seg', epoch_ndx)\n",
    "        # self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "        # self.logImages(epoch_ndx, 'val', val_dl)\n",
    "\n",
    "#     if epoch_ndx % pred_cadence ==0:\n",
    "#         predMetrics_t = doPrediction(epoch_ndx, VAL_FILES, batch_size=1, patch_size=(128,256,256))\n",
    "#         score = logMetrics(epoch_ndx, 'pred', predMetrics_t)\n",
    "        \n",
    "trn_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1626940740398,
     "user": {
      "displayName": "이정오",
      "photoUrl": "",
      "userId": "04886549528950007370"
     },
     "user_tz": -540
    },
    "id": "gB5Zd_kn8bes",
    "outputId": "ef387e03-fc9d-4803-ec21-91aa4b6f33ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $BASE_DIR/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMGTa8s3ioYzkVicewqKqGu",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BoneSegmentation_window_1300300_FN0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
